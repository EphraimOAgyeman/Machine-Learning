{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87794d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides   \n",
       "0            7.4              0.70         0.00             1.9      0.076  \\\n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates   \n",
       "0                 11.0                  34.0   0.9978  3.51       0.56  \\\n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "red_wine_df = pd.read_csv('red-wine.csv')\n",
    "red_wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e584fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>10.8</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.171</td>\n",
       "      <td>27.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.99820</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.76</td>\n",
       "      <td>10.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0.095</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.99854</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>9.1</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.063</td>\n",
       "      <td>13.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.99516</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.84</td>\n",
       "      <td>11.7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>10.2</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.053</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.99820</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.42</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides   \n",
       "1109           10.8             0.470         0.43            2.10      0.171  \\\n",
       "1032            8.1             0.820         0.00            4.10      0.095   \n",
       "1002            9.1             0.290         0.33            2.05      0.063   \n",
       "487            10.2             0.645         0.36            1.80      0.053   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates   \n",
       "1109                 27.0                  66.0  0.99820  3.17       0.76  \\\n",
       "1032                  5.0                  14.0  0.99854  3.36       0.53   \n",
       "1002                 13.0                  27.0  0.99516  3.26       0.84   \n",
       "487                   5.0                  14.0  0.99820  3.17       0.42   \n",
       "\n",
       "      alcohol  quality  \n",
       "1109     10.8        6  \n",
       "1032      9.6        5  \n",
       "1002     11.7        7  \n",
       "487      10.0        6  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training and validation splits\n",
    "df_train = red_wine_df.sample(frac=0.7, random_state=0)\n",
    "df_valid = red_wine_df.drop(df_train.index)\n",
    "df_train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb063436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale to [0, 1]\n",
    "max_ = df_train.max(axis=0)\n",
    "min_ = df_train.min(axis=0)\n",
    "df_train = (df_train - min_) / (max_ - min_)\n",
    "df_valid = (df_valid - min_) / (max_ - min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec6e6ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X_train = df_train.drop('quality', axis=1)\n",
    "X_valid = df_valid.drop('quality', axis=1)\n",
    "y_train = df_train['quality']\n",
    "y_valid = df_valid['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d73f51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1119, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67241ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35c5c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0.002, # minimium amount of change to count as an improvement\n",
    "    patience=100, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3dfd9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(units=300, activation='relu', input_shape=[11]),\n",
    "    layers.Dropout(rate=0.3),# to break wrong clustering of computations for better analysis\n",
    "    layers.BatchNormalization(),# to normalize the batch data that comes in the layer\n",
    "    \n",
    "    layers.Dense(units=400, activation='relu'),\n",
    "    layers.Dropout(rate=0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "    layers.Dense(units=400, activation='relu'),\n",
    "    layers.Dropout(rate=0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "    layers.Dense(units=200, activation='relu'),\n",
    "    layers.Dropout(rate=0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "    layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec11aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\", # tunes the model to find the least loss at every step\n",
    "    loss=\"mae\", # calculates the difference between the true values and predicted values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ae450cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0850 - val_loss: 0.0886\n",
      "Epoch 2/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0881 - val_loss: 0.0880\n",
      "Epoch 3/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0886 - val_loss: 0.0926\n",
      "Epoch 4/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0886 - val_loss: 0.0877\n",
      "Epoch 5/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0906 - val_loss: 0.0874\n",
      "Epoch 6/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0876 - val_loss: 0.0904\n",
      "Epoch 7/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0875 - val_loss: 0.0885\n",
      "Epoch 8/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0888 - val_loss: 0.0917\n",
      "Epoch 9/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0876 - val_loss: 0.0884\n",
      "Epoch 10/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0864 - val_loss: 0.0891\n",
      "Epoch 11/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0854 - val_loss: 0.0901\n",
      "Epoch 12/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0873 - val_loss: 0.0952\n",
      "Epoch 13/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0882 - val_loss: 0.0927\n",
      "Epoch 14/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0879 - val_loss: 0.0927\n",
      "Epoch 15/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0858 - val_loss: 0.0896\n",
      "Epoch 16/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0860 - val_loss: 0.0999\n",
      "Epoch 17/200\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0858 - val_loss: 0.0918\n",
      "Epoch 18/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0911 - val_loss: 0.1044\n",
      "Epoch 19/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0938 - val_loss: 0.1055\n",
      "Epoch 20/200\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0873 - val_loss: 0.0905\n",
      "Epoch 21/200\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0891 - val_loss: 0.0915\n",
      "Epoch 22/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0875 - val_loss: 0.1007\n",
      "Epoch 23/200\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0860 - val_loss: 0.0956\n",
      "Epoch 24/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0853 - val_loss: 0.0958\n",
      "Epoch 25/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0891 - val_loss: 0.1144\n",
      "Epoch 26/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0886 - val_loss: 0.0930\n",
      "Epoch 27/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0855 - val_loss: 0.0949\n",
      "Epoch 28/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0875 - val_loss: 0.0905\n",
      "Epoch 29/200\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0889 - val_loss: 0.0914\n",
      "Epoch 30/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0863 - val_loss: 0.0908\n",
      "Epoch 31/200\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0856 - val_loss: 0.0977\n",
      "Epoch 32/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0868 - val_loss: 0.0918\n",
      "Epoch 33/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0845 - val_loss: 0.0940\n",
      "Epoch 34/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0848 - val_loss: 0.0915\n",
      "Epoch 35/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0829 - val_loss: 0.0942\n",
      "Epoch 36/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0832 - val_loss: 0.0916\n",
      "Epoch 37/200\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0871 - val_loss: 0.0941\n",
      "Epoch 38/200\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0896 - val_loss: 0.0925\n",
      "Epoch 39/200\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0859 - val_loss: 0.0871\n",
      "Epoch 40/200\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0854 - val_loss: 0.0887\n",
      "Epoch 41/200\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0831 - val_loss: 0.0918\n",
      "Epoch 42/200\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0828 - val_loss: 0.0901\n",
      "Epoch 43/200\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0850 - val_loss: 0.0862\n",
      "Epoch 44/200\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0812 - val_loss: 0.0865\n",
      "Epoch 45/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0856 - val_loss: 0.0889\n",
      "Epoch 46/200\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0864 - val_loss: 0.0943\n",
      "Epoch 47/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0834 - val_loss: 0.0951\n",
      "Epoch 48/200\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0864 - val_loss: 0.0867\n",
      "Epoch 49/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0812 - val_loss: 0.0879\n",
      "Epoch 50/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0854 - val_loss: 0.0910\n",
      "Epoch 51/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0835 - val_loss: 0.0920\n",
      "Epoch 52/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0811 - val_loss: 0.0883\n",
      "Epoch 53/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0855 - val_loss: 0.0926\n",
      "Epoch 54/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0839 - val_loss: 0.0955\n",
      "Epoch 55/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0866 - val_loss: 0.0938\n",
      "Epoch 56/200\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0840 - val_loss: 0.0900\n",
      "Epoch 57/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0847 - val_loss: 0.0915\n",
      "Epoch 58/200\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0846 - val_loss: 0.0868\n",
      "Epoch 59/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0907 - val_loss: 0.0902\n",
      "Epoch 60/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0875 - val_loss: 0.0965\n",
      "Epoch 61/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0833 - val_loss: 0.0888\n",
      "Epoch 62/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0798 - val_loss: 0.0872\n",
      "Epoch 63/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0844 - val_loss: 0.0962\n",
      "Epoch 64/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0856 - val_loss: 0.0916\n",
      "Epoch 65/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0844 - val_loss: 0.0884\n",
      "Epoch 66/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0856 - val_loss: 0.0881\n",
      "Epoch 67/200\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0826 - val_loss: 0.0907\n",
      "Epoch 68/200\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0862 - val_loss: 0.0898\n",
      "Epoch 69/200\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0836 - val_loss: 0.0930\n",
      "Epoch 70/200\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0868 - val_loss: 0.0908\n",
      "Epoch 71/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0860 - val_loss: 0.0949\n",
      "Epoch 72/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0835 - val_loss: 0.0892\n",
      "Epoch 73/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0865 - val_loss: 0.0896\n",
      "Epoch 74/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0836 - val_loss: 0.0863\n",
      "Epoch 75/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0821 - val_loss: 0.0903\n",
      "Epoch 76/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0833 - val_loss: 0.0909\n",
      "Epoch 77/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0824 - val_loss: 0.0955\n",
      "Epoch 78/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0826 - val_loss: 0.0984\n",
      "Epoch 79/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0861 - val_loss: 0.0905\n",
      "Epoch 80/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0837 - val_loss: 0.0988\n",
      "Epoch 81/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0853 - val_loss: 0.0910\n",
      "Epoch 82/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0785 - val_loss: 0.0913\n",
      "Epoch 83/200\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0828 - val_loss: 0.0905\n",
      "Epoch 84/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0832 - val_loss: 0.0893\n",
      "Epoch 85/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0838 - val_loss: 0.0958\n",
      "Epoch 86/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0839 - val_loss: 0.0955\n",
      "Epoch 87/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0816 - val_loss: 0.0999\n",
      "Epoch 88/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0891 - val_loss: 0.1015\n",
      "Epoch 89/200\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0885 - val_loss: 0.0889\n",
      "Epoch 90/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0869 - val_loss: 0.0924\n",
      "Epoch 91/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0829 - val_loss: 0.0943\n",
      "Epoch 92/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0843 - val_loss: 0.0916\n",
      "Epoch 93/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0832 - val_loss: 0.0914\n",
      "Epoch 94/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0788 - val_loss: 0.0897\n",
      "Epoch 95/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0808 - val_loss: 0.0920\n",
      "Epoch 96/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0822 - val_loss: 0.0908\n",
      "Epoch 97/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0837 - val_loss: 0.0949\n",
      "Epoch 98/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0821 - val_loss: 0.0941\n",
      "Epoch 99/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0837 - val_loss: 0.0925\n",
      "Epoch 100/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0799 - val_loss: 0.0901\n",
      "Epoch 101/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0795 - val_loss: 0.0903\n",
      "Epoch 102/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0813 - val_loss: 0.0916\n",
      "Epoch 103/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0810 - val_loss: 0.0945\n",
      "Epoch 104/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0803 - val_loss: 0.0951\n",
      "Epoch 105/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0813 - val_loss: 0.0962\n",
      "Epoch 106/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0792 - val_loss: 0.0939\n",
      "Epoch 107/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0823 - val_loss: 0.1018\n",
      "Epoch 108/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0815 - val_loss: 0.0916\n",
      "Epoch 109/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0807 - val_loss: 0.0896\n",
      "Epoch 110/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0813 - val_loss: 0.0905\n",
      "Epoch 111/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0839 - val_loss: 0.0950\n",
      "Epoch 112/200\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0817 - val_loss: 0.0904\n",
      "Epoch 113/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0790 - val_loss: 0.0943\n",
      "Epoch 114/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0793 - val_loss: 0.0891\n",
      "Epoch 115/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0801 - val_loss: 0.1020\n",
      "Epoch 116/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0813 - val_loss: 0.0918\n",
      "Epoch 117/200\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.0824 - val_loss: 0.0899\n",
      "Epoch 118/200\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0806 - val_loss: 0.0907\n",
      "Epoch 119/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0822 - val_loss: 0.0933\n",
      "Epoch 120/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0800 - val_loss: 0.0915\n",
      "Epoch 121/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0816 - val_loss: 0.0898\n",
      "Epoch 122/200\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0825 - val_loss: 0.0898\n",
      "Epoch 123/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0831 - val_loss: 0.0875\n",
      "Epoch 124/200\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0820 - val_loss: 0.0913\n",
      "Epoch 125/200\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0795 - val_loss: 0.0918\n",
      "Epoch 126/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0860 - val_loss: 0.1007\n",
      "Epoch 127/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0826 - val_loss: 0.0955\n",
      "Epoch 128/200\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0810 - val_loss: 0.0935\n",
      "Epoch 129/200\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0804 - val_loss: 0.0928\n",
      "Epoch 130/200\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0815 - val_loss: 0.1090\n",
      "Epoch 131/200\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0833 - val_loss: 0.0938\n",
      "Epoch 132/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0818 - val_loss: 0.0929\n",
      "Epoch 133/200\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0796 - val_loss: 0.0881\n",
      "Epoch 134/200\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0791 - val_loss: 0.0963\n",
      "Epoch 135/200\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0803 - val_loss: 0.0892\n",
      "Epoch 136/200\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0837 - val_loss: 0.0929\n",
      "Epoch 137/200\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.0818 - val_loss: 0.0934\n",
      "Epoch 138/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0812 - val_loss: 0.0921\n",
      "Epoch 139/200\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.0806 - val_loss: 0.0940\n",
      "Epoch 140/200\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0794 - val_loss: 0.1006\n",
      "Epoch 141/200\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0798 - val_loss: 0.0956\n",
      "Epoch 142/200\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.0807 - val_loss: 0.0939\n",
      "Epoch 143/200\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.0829 - val_loss: 0.0985\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=50,\n",
    "    epochs=200,\n",
    "    \n",
    "    callbacks=[early_stopping], # put your callbacks in a list\n",
    "    #verbose=0,  # turn off training log\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e2435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
